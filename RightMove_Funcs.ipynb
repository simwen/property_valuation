{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaeafaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import geopy\n",
    "from geopy import distance\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb96a16",
   "metadata": {},
   "source": [
    "### First Scraper\n",
    "\n",
    "**Loops through all n rightmove results pages extracting link, price and id of each property.**\n",
    "\n",
    "1. First we define the parameters of our search i.e. the max/min price and the radius around the station.\n",
    "2. Next, since the URL changes for page 1 vs pages 2+, we reconfigure the request accordingly using `if` and `elif`. The URLs have the parameters of our search inserted. \n",
    "3. Requests.get fetches the specified webpage. r objects have `.text` attributes which returns the webpage's raw html.\n",
    "4. BeautifulSoup is a package which parses html and returns a `soup` object.\n",
    "5. `find_all` takes a html tag as an argument (\"div\" here). Any argument that’s not recognized (e.g. class_) will be turned into a filter on a tag’s attributes. Here the argument class_, is used to filter against each tag’s 'class_' attribute which identifies a new property.\n",
    "6. Looping through the apartments, we extract the relevant information this time using `find` and looking for the relevant info indicated by 'class_' again.\n",
    "7. Appending the info at the end of each loop means we compile all the info across the webpages.\n",
    "\n",
    "https://www.rightmove.co.uk/property-for-sale/find.html?locationIdentifier=STATION%5E6095&maxPrice=800000&minPrice=400000&radius=3.0&sortType=6&index=24&propertyTypes=detached%2Cflat%2Csemi-detached%2Cterraced&secondaryDisplayPropertyType=housesandflats&includeSSTC=false&mustHave=&dontShow=&furnishTypes=&keywords="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aacd6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the rightmove property search results webpages\n",
    "# Returns link, price and id of properties within a set radius from Marylebone station within the specified price bounds\n",
    "\n",
    "def scrape_results_page(minPrice=450000,maxPrice=800000,radius=3,noPages=42):\n",
    "    all_apartment_links, all_price, all_id_no = [], [], [] # stores apartment links, listing prices and ids\n",
    "    \n",
    "    for i in range(noPages):\n",
    "        if i==0:\n",
    "            r= requests.get(f'https://www.rightmove.co.uk/property-for-sale/find.html?locationIdentifier=STATION%5E7658&maxPrice={maxPrice}&minPrice={minPrice}&radius={radius}&sortType=6&propertyTypes=detached%2Cflat%2Csemi-detached%2Cterraced&secondaryDisplayPropertyType=housesandflats&includeSSTC=false&mustHave=&dontShow=&furnishTypes=&keywords=')\n",
    "        else:\n",
    "            r = ''\n",
    "            while r == '':\n",
    "                try:\n",
    "                    r = requests.get(f'https://www.rightmove.co.uk/property-for-sale/find.html?locationIdentifier=STATION%5E7658&maxPrice={maxPrice}&minPrice={minPrice}&radius={radius}&sortType=6&index={i*24}&propertyTypes=detached%2Cflat%2Csemi-detached%2Cterraced&secondaryDisplayPropertyType=housesandflats&includeSSTC=false&mustHave=&dontShow=&furnishTypes=&keywords=')\n",
    "                    break\n",
    "                except:\n",
    "                    print(f'Connection refused by the server on page {i+1}... sleeping for 3 seconds')\n",
    "                    time.sleep(3)\n",
    "                    print(\"Was a nice sleep, now let me continue...\")\n",
    "                    continue\n",
    "            #r= requests.get(f'https://www.rightmove.co.uk/property-for-sale/find.html?locationIdentifier=STATION%5E6095&maxPrice={maxPrice}&minPrice={minPrice}&radius={radius}&sortType=6&index={i*24}&propertyTypes=detached%2Cflat%2Csemi-detached%2Cterraced&secondaryDisplayPropertyType=housesandflats&includeSSTC=false&mustHave=&dontShow=&furnishTypes=&keywords=')\n",
    "\n",
    "        soup = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "        apartments = soup.find_all(\"div\", class_=\"l-searchResult is-list\")\n",
    "\n",
    "        for i in range(len(apartments)):\n",
    "\n",
    "            # tracks which apartment we are on in the page\n",
    "            apartment_no = apartments[i]\n",
    "\n",
    "            # append link\n",
    "            apartment_info = apartment_no.find(\"a\", class_=\"propertyCard-link\")\n",
    "            link = \"https://www.rightmove.co.uk\" + apartment_info.attrs[\"href\"]\n",
    "            all_apartment_links.append(link)\n",
    "\n",
    "            # append price\n",
    "            price = (\n",
    "                apartment_no.find(\"div\", class_=\"propertyCard-priceValue\")\n",
    "                .get_text()\n",
    "                .strip()\n",
    "            )\n",
    "            all_price.append(price)\n",
    "\n",
    "            # append id\n",
    "            id_no = (\n",
    "                apartment_no.find(\"div\", id_=\"property-*\")\n",
    "                #.get_text()\n",
    "                #.strip()\n",
    "            )\n",
    "            all_id_no.append(id_no)\n",
    "\n",
    "    return r.ok, all_apartment_links, all_price, all_id_no"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b7187",
   "metadata": {},
   "source": [
    "### Second Scraper\n",
    "\n",
    "**Loops through all listing specific weblinks identified above extracting data on each property.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfd00eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the links from scrape_results_page function above to scrapes individual property listings\n",
    "# Gathers data relevant to a property's value such as its features and location\n",
    "\n",
    "def scrape_listings(date_from, links):\n",
    "\n",
    "    all_links, all_features, all_prices, all_statname, all_statdist =[], [], [], [], []\n",
    "    all_outcodes, all_postcodes, all_centralities, all_dates = [], [], [], []\n",
    "    \n",
    "    for i in range(len(links)):\n",
    "        # Progress tracker\n",
    "        if len(links) > 20:\n",
    "            if i % (len(links)//20) == 0:\n",
    "                percent = round(i*100/len(links))\n",
    "                print(f'Code is {percent}% completed')\n",
    "\n",
    "        r= requests.get(links[i])\n",
    "\n",
    "        soup = BeautifulSoup(r.text, 'lxml')\n",
    "        \n",
    "        # Append Date uploaded\n",
    "        link = soup.find_all('script')[6]    \n",
    "\n",
    "        jsonobj = json.loads(link.text[25:]) # converts json into dictionary  \n",
    "        date = jsonobj.get(\"analyticsInfo\").get('analyticsProperty').get('added')\n",
    "        \n",
    "        if int(date) <= date_from:   # Skip if from an already scraped date\n",
    "            continue\n",
    "        all_dates.append(date)\n",
    "        \n",
    "        # Append variables of interest\n",
    "        # weblink\n",
    "        all_links.append(links[i])\n",
    "\n",
    "        # physical features\n",
    "        features = (\n",
    "            soup.find_all(\"dd\", class_=\"_1hV1kqpVceE9m-QrX_hWDN\")\n",
    "        )\n",
    "        features =[str(i).replace('<dd class=\"_1hV1kqpVceE9m-QrX_hWDN\">', '').replace('</dd>', '')\n",
    "         .replace('<dd class=\"_1hV1kqpVceE9m-QrX_hWDN _2SpNNVW0fTYoFvPDmhKSt8 _3ixAp8byA3wC3qvii8d-kg\">' , '') \n",
    "         for i in features]\n",
    "        features =[i for i in features if \"<dd class\" not in i ]\n",
    "        all_features.append(features)\n",
    "\n",
    "        # price\n",
    "        price = (\n",
    "            soup.find('input').attrs['value']\n",
    "            #.get_text()\n",
    "            #.strip()\n",
    "        )\n",
    "        all_prices.append(int(price.replace(\",\",\"\")))\n",
    "\n",
    "        # postcodes   \n",
    "        outcode = jsonobj.get(\"propertyData\").get('address').get('outcode')\n",
    "        postcode = outcode + jsonobj.get(\"propertyData\").get('address').get('incode')\n",
    "        all_outcodes.append(outcode)\n",
    "        all_postcodes.append(postcode)    \n",
    "\n",
    "        # distance to centre of London\n",
    "        latitude = float(jsonobj.get(\"propertyData\").get('location').get('latitude'))\n",
    "        longitude = float(jsonobj.get(\"propertyData\").get('location').get('longitude'))\n",
    "        coords = (latitude, longitude)\n",
    "        charingX = (51.507602, -0.127816)\n",
    "        centrality = geopy.distance.geodesic(coords, charingX).km\n",
    "        all_centralities.append(centrality)\n",
    "\n",
    "        # stations\n",
    "        statdist = (\n",
    "            soup.find(\"span\", class_=\"_1ZY603T1ryTT3dMgGkM7Lg\")\n",
    "            .get_text()\n",
    "            .strip()\n",
    "        )\n",
    "        all_statdist.append(float(statdist.replace(\" miles\",\"\")))\n",
    "\n",
    "        statname = (\n",
    "            soup.find(\"span\", class_=\"cGDiWU3FlTjqSs-F1LwK4\")\n",
    "            .get_text()\n",
    "            .strip()\n",
    "        )\n",
    "        all_statname.append(statname)\n",
    "\n",
    "    return r.ok, all_links,all_features, all_prices, all_statname, all_statdist, all_outcodes,all_postcodes,all_centralities, all_dates "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db04129",
   "metadata": {},
   "source": [
    "### Property Valuer\n",
    "\n",
    "**Takes link to chosen property and predicts values using best trained models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c24740e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_scrape(year_month):\n",
    "    # Scraping the data\n",
    "    r= requests.get('https://www.plumplot.co.uk/London-house-prices.html')\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "    rows = soup.find_all(\"tr\")\n",
    "\n",
    "    outcode, avg_price = [], []\n",
    "    for i in range(4,960):\n",
    "        outcode.append(rows[i].find(lambda tag: tag.name == 'td' and \n",
    "                  tag.get('class') == ['mh2']).text)\n",
    "        outcode[i-4] = outcode[i-4].replace(\" \", \"\")\n",
    "        avg_price.append(rows[i].find(\"td\", class_=\"mh2 text-right\").text)\n",
    "        avg_price[i-4] = int(avg_price[i-4][:-1])\n",
    "\n",
    "    area_df = pd.DataFrame.from_dict({\"Outcode3\": outcode, \"Avg_price\": avg_price})\n",
    "    area_df.to_csv(f'{location}/data/area_df_{year_month}.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
